## **DAS6 - stores Weekly options tick data for NIFTY and BANKNIFTY**

## **Sequence to run DAS5 scripts**:
1. Yearly activity:  
1.1 Update the NSE trading holidays for the year in [tradingHolidays.csv](https://github.com/rthennan/ZerodhaWebsocket/blob/main/DAS6/expiryGenerator/tradingHolidays.csv)  
1.2 expiryGenerator\expSuffGenerator.py - Once a year, before the year begins.
3. accessTokenReqDAS6.py (Skip this if you have already run it as a part of DAS5).
4. DAS6_MasterV1.

## **expiryGenerator\expSuffGenerator.py:**  
- To be run from the expSuffGenerator directory.
- The Instrument names for NIFTY and BANKNIFTY weekly options have a naming convention as stated [here](https://kite.trade/forum/discussion/5574/change-in-format-of-weekly-options-instruments).
- I was initially updating the table prefix names and the instrument list on a weekly basis, every Thursday.
- Missing this activity meant I wouldn't get the Weekly options data until I update the current week's prefix. Hence this automation.
- The reason it has to be done yearly and not just once is cause the NSE trading holidays list for any year is provided just before the year begins.  The instrument name prefix for a week could change if that week's Thursday is a trading holiday.
- So I get the trading holidays list for the new year and complete the activity before the first trading day of the year.

## **accessTokenReqDAS6:**
Refer the [Access Tokens section](https://github.com/rthennan/ZerodhaWebsocket/tree/main/DAS5#accesstoken---accesstokenreq-and-accesstokenreqdas6) in DAS5's Readme.

## **DAS6_MasterV1 's tasks**:
- lookupTab.
- DAS6_NFOmainV1 (as NFO) and DAS6_BNFOmainV1 (As BNFO) || Processed Parallely.
- Wait for these two to finish. (They will run till 15:35).
- DAS6_backUpNFO_V1 (as backUpNFOFULL).
- DAS6_backUpBNFO_V1 (as backUpBNFOFULL).

## **Explanation for DAS6_MasterV1's individual steps:**  
### **lookupTab.py**
- Unlike the one in DAS5, this lookup task just deletes empty tables from the previous day.
- This is cause DAS6_NFOmainV1 and DAS6_BNFOmainV1 together would create atleast 400 tables everyday. More if the market is trending.
- And if the market is moving in a different range the next day, a new set of tables are created of course.
- Since the Daily tables are backed up at EOD, these tables are pointless and hence deleted.  
- Downloads the latest list of instrument_token along with ticker symbols from https://api.kite.trade/instruments and stores it in the lookup_tables folder.

### **DAS6_NFOmainV1:**
- Uses apikey1 and apisec1
- The following actions are repeated till the market closes - 15:35 IST:
  - Connects to Zerodha Kite API (not websocket) and gets the Last Traded Price of Nifty Index.
  - Creates a list of strike prices from LTP-2500 to LT+2500, with a gap of 100 between each.
    example: If NiFTY LTP is 17000, a range is generated from 14500 to 19500.  - range(14500,19500,100) - 50 strike prices.
  - These prices are suffixed with CE and PE (100 strike options) and then prefixed with the currentExpiry and nextExpiry codes (lookup_tables\nfo_simmonsLookup.csv) generated by expSuffGenerator (200 instruments).
  - Tables are created for these instruments.
  - An instrument token list(nfoTokenList.csv) and an instrument_token:TableName dictionary (nfoTokenSymbol.npy) is created for the instruments. Similar to [lookupins.py in DAS5](https://github.com/rthennan/ZerodhaWebsocket/blob/main/DAS5/README.md#lookupinspy)
  - DAS6_NFO_Full_V1 is importedd and run.
  - DAS6_NFO_Full_V1 is killed after 5 minutes.
 - This ensures tick data for most relevant option strikes are collected even during a heavily trending market.

### **DAS6_BNFOmainV1:**
- Uses apikey1 and apisec1
- Actions similar to DAS6_NFOmainV1, but for BankNifty Options

## **DAS6_backUpNFO_V1 and DAS6_backUpBNFO_V1:**
- Backup 'Daily' tables to the main database.

### **DAS6_WatchDog:**  
- Nifty and BankNifty options are reconnected every 5 minutes to get a range of Option chains close to the index's current price. 
- This watchdog checks if the connections were re-established successfully.
- Not integrated with the main DAS trheads (on purpose) and has to be run separately. 
